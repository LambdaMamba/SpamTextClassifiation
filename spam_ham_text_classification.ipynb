{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>convey my regards to him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>청_  anyway  many good evenings to u  s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>my sort code is  and acc no is   the bank is n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>sorry i din lock my keypad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>hi babe its chloe  how r u  i was smashed on s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class                                               Text\n",
       "0   ham                           convey my regards to him\n",
       "1   ham             청_  anyway  many good evenings to u  s\n",
       "2   ham  my sort code is  and acc no is   the bank is n...\n",
       "3   ham                        sorry i din lock my keypad \n",
       "4  spam  hi babe its chloe  how r u  i was smashed on s..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"sms_train.csv\")\n",
    "df_train.columns = ['Class', 'Text']\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in all messages 7766\n"
     ]
    }
   ],
   "source": [
    "# make all volabulary list\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "df_train['Text'].head()\n",
    "\n",
    "for sms in df_train['Text']:\n",
    "    for word in str(sms).split():\n",
    "        vocabulary.append(word)\n",
    "\n",
    "vocabulary = list(set(vocabulary))\n",
    "#print(vocabulary)\n",
    "N_vocabulary = len(vocabulary)\n",
    "\n",
    "print(\"Unique words in all messages\", N_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in spam 15595\n",
      "Number of words in ham 56168\n"
     ]
    }
   ],
   "source": [
    "spam_messages = df_train[df_train['Class'] == 'spam']\n",
    "ham_messages = df_train[df_train['Class'] == 'ham']\n",
    "\n",
    "n_words_per_spam_message = spam_messages['Text'].str.split().apply(len)\n",
    "\n",
    "spam_total = np.sum(n_words_per_spam_message)\n",
    "\n",
    "print(\"Number of words in spam\", spam_total)\n",
    "\n",
    "n_words_per_ham_message = ham_messages['Text'].str.split().apply(len)\n",
    "\n",
    "ham_total = np.sum(n_words_per_ham_message)\n",
    "print(\"Number of words in ham\", ham_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words \n",
    "\n",
    "word_count_per_sms = {unique_word: [0] * len(df_train['Text']) for unique_word in vocabulary}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop 123\n",
      "   angels  shaved  ollu  green  wamma  09061790126  sashimi  moseley  pop  \\\n",
      "0       0       0     0      0      0            0        0        0    0   \n",
      "1       0       0     0      0      0            0        0        0    0   \n",
      "2       0       0     0      0      0            0        0        0    0   \n",
      "3       0       0     0      0      0            0        0        0    0   \n",
      "4       0       0     0      0      0            0        0        0    0   \n",
      "\n",
      "   4goten  ...  colleg  sorting  anyways  miles  round  a30  large  arent  \\\n",
      "0       0  ...       0        0        0      0      0    0      0      0   \n",
      "1       0  ...       0        0        0      0      0    0      0      0   \n",
      "2       0  ...       0        0        0      0      0    0      0      0   \n",
      "3       0  ...       0        0        0      0      0    0      0      0   \n",
      "4       0  ...       0        0        0      0      0    0      0      0   \n",
      "\n",
      "   your  dearer  \n",
      "0     0       0  \n",
      "1     0       0  \n",
      "2     0       0  \n",
      "3     0       0  \n",
      "4     1       0  \n",
      "\n",
      "[5 rows x 7766 columns]\n"
     ]
    }
   ],
   "source": [
    "# word_count_per_sms[word][i] = num   means i-th sms messages have num words.\n",
    "\n",
    "wordlist = []\n",
    "for index, sms in enumerate(df_train['Text']):\n",
    "    for word in str(sms).split():\n",
    "        word_count_per_sms[word][index] = word_count_per_sms[word][index] + 1\n",
    "        # write code here to make the bag of words dictionary.\n",
    "        \n",
    "\n",
    "print(\"Number of stop\" , np.sum(word_count_per_sms[\"stop\"]))\n",
    "# convert dictionary to pandas dataframe\n",
    "word_counts = pd.DataFrame(word_count_per_sms)\n",
    "\n",
    "print(word_counts.head())\n",
    "\n",
    "df_train_wcount = pd.concat([df_train, word_counts], axis=1)\n",
    "df_train_wcount.head()\n",
    "\n",
    "# Isolating spam and ham messages \n",
    "spam_messages = df_train_wcount[df_train_wcount['Class'] == 'spam']\n",
    "ham_messages = df_train_wcount[df_train_wcount['Class'] == 'ham']\n",
    "\n",
    "#print(spam_messages.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of call in spam 282\n",
      "Number of call in ham 190\n"
     ]
    }
   ],
   "source": [
    "callspam = np.sum(spam_messages[\"call\"])\n",
    "callham = np.sum(ham_messages[\"call\"])\n",
    "\n",
    "print(\"Number of call in spam\" , callspam)\n",
    "print(\"Number of call in ham\" , callham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(call|spam) is  0.01211420743975001\n",
      "P(call|ham) is  0.0029874558138079893\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "# Initiate parameters\n",
    "# p_wi_spam[wi] means P(wi|Spam)\n",
    "p_wi_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "p_wi_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "for word in vocabulary:\n",
    "    n_wi_spam = np.sum(spam_messages[word])\n",
    "    p_wi_spam[word] = (n_wi_spam + alpha)/(spam_total + alpha*N_vocabulary)\n",
    "    # calculate P(wi|Spam) and P(wi|Ham) under alpha = 1\n",
    "    # P(wi|Spam) = (# of times the word wi occurs in spam message) + alpha / n_spam + alpha * n_volabulary\n",
    "    # P(wi|Ham) = (# of times the word wi occurs in ham message) + alpha / n_ham + alpha * n_volabulary\n",
    "    \n",
    "for word in vocabulary:\n",
    "    n_wi_ham = np.sum(ham_messages[word])\n",
    "    p_wi_ham[word] = (n_wi_ham + alpha)/(ham_total + alpha*N_vocabulary)\n",
    "    # calculate P(wi|Spam) and P(wi|Ham) under alpha = 1\n",
    "    # P(wi|Spam) = (# of times the word wi occurs in spam message) + alpha / n_spam + alpha * n_volabulary\n",
    "    # P(wi|Ham) = (# of times the word wi occurs in ham message) + alpha / n_ham + alpha * n_volabulary\n",
    "    \n",
    "print(\"P(call|spam) is \", p_wi_spam[\"call\"])\n",
    "print(\"P(call|ham) is \", p_wi_ham[\"call\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(spam) is  0.21731254267519473\n",
      "p(ham) is  0.7826874573248053\n"
     ]
    }
   ],
   "source": [
    "p_spam = spam_total/(spam_total + ham_total)\n",
    "\n",
    "p_ham = ham_total/(spam_total + ham_total)\n",
    "\n",
    "print(\"p(spam) is \", p_spam)\n",
    "print(\"p(ham) is \", p_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam|free, video) is  1.6119161193766106e-06\n",
      "P(ham|free, video) is  3.216872903478259e-08\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# logP_wi_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "# logP_wi_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "# for word in vocabulary:\n",
    "#     logP_wi_spam[word] = math.log(p_wi_spam[word])\n",
    "    \n",
    "# for word in vocabulary:\n",
    "#     logP_wi_ham[word] = math.log(p_wi_ham[word])\n",
    "    \n",
    "# P_spam_free_video = math.log(p_spam) + logP_wi_spam[\"free\"] + logP_wi_spam[\"video\"]\n",
    "\n",
    "# P_ham_free_video = math.log(p_ham) + logP_wi_ham[\"free\"] + logP_wi_ham[\"video\"]\n",
    "\n",
    "\n",
    "# print(\"P(spam|free, video) is \", P_spam_free_video)\n",
    "# print(\"P(ham|free, video) is \", P_ham_free_video)\n",
    "\n",
    "\n",
    "P_spam_free_video = math.exp(math.log(p_spam) + math.log(p_wi_spam[\"free\"]) + math.log(p_wi_spam[\"video\"]))\n",
    "\n",
    "P_ham_free_video = math.exp(math.log(p_ham) + math.log(p_wi_ham[\"free\"]) + math.log(p_wi_ham[\"video\"]))\n",
    "\n",
    "\n",
    "print(\"P(spam|free, video) is \", P_spam_free_video)\n",
    "print(\"P(ham|free, video) is \", P_ham_free_video)\n",
    "\n",
    "\n",
    "# calculate P(Spam|w1, w2, w3, ...) and P(Ham|w1, w2, w3, ...) and compare them\n",
    "\n",
    "# if P(Spam|w1, w2, w3, ...) >= P(Ham|w1, w2, w3, ...) : Spam message\n",
    "# if P(Spam|w1, w2, w3, ...) < P(Ham|w1, w2, w3, ...) : Ham message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 561), ('a', 315), ('call', 282), ('책', 249), ('you', 238), ('your', 219), ('free', 175), ('2', 168), ('now', 166), ('the', 163), ('for', 160), ('or', 146), ('u', 142), ('txt', 138), ('is', 133), ('4', 120), ('ur', 117), ('on', 113), ('have', 108), ('from', 107), ('mobile', 103), ('text', 98), ('claim', 95), ('1', 94), ('and', 91), ('stop', 90), ('with', 88), ('reply', 83), ('of', 81), ('s', 76), ('prize', 75), ('www', 74), ('this', 73), ('t', 71), ('only', 66), ('get', 66), ('in', 65), ('are', 65), ('our', 64), ('won', 63), ('just', 63), ('no', 61), ('cash', 60), ('new', 60), ('win', 57), ('send', 57), ('nokia', 55), ('i', 53), ('150p', 53), ('urgent', 52), ('uk', 51), ('out', 49), ('week', 49), ('tone', 48), ('c', 47), ('contact', 47), ('com', 46), ('50', 46), ('service', 45), ('be', 43), ('16', 42), ('18', 41), ('customer', 41), ('3', 40), ('we', 40), ('phone', 39), ('please', 39), ('msg', 38), ('guaranteed', 38), ('per', 37), ('will', 36), ('100', 36), ('co', 35), ('500', 35), ('cs', 34), ('min', 34), ('who', 34), ('been', 33), ('draw', 33), ('1000', 32), ('awarded', 32), ('chat', 32), ('by', 31), ('line', 31), ('every', 30), ('wk', 30), ('sms', 30), ('as', 30), ('camera', 30), ('it', 29), ('150ppm', 29), ('latest', 29), ('shows', 28), ('message', 28), ('mins', 27), ('number', 27), ('chance', 26), ('landline', 26), ('all', 26), ('me', 25)]\n",
      "['to', 'a', 'call', '책', 'you', 'your', 'free', '2', 'now', 'the', 'for', 'or', 'u', 'txt', 'is', '4', 'ur', 'on', 'have', 'from', 'mobile', 'text', 'claim', '1', 'and', 'stop', 'with', 'reply', 'of', 's', 'prize', 'www', 'this', 't', 'only', 'get', 'in', 'are', 'our', 'won', 'just', 'no', 'cash', 'new', 'win', 'send', 'nokia', 'i', '150p', 'urgent', 'uk', 'out', 'week', 'tone', 'c', 'contact', 'com', '50', 'service', 'be', '16', '18', 'customer', '3', 'we', 'phone', 'please', 'msg', 'guaranteed', 'per', 'will', '100', 'co', '500', 'cs', 'min', 'who', 'been', 'draw', '1000', 'awarded', 'chat', 'by', 'line', 'every', 'wk', 'sms', 'as', 'camera', 'it', '150ppm', 'latest', 'shows', 'message', 'mins', 'number', 'chance', 'landline', 'all', 'me']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "allspam_messages = df_train[df_train['Class'] == 'spam']\n",
    "allham_messages = df_train[df_train['Class'] == 'ham']\n",
    "\n",
    "#find most common spam words\n",
    "\n",
    "most_occur_spam = Counter(\" \".join(allspam_messages[\"Text\"]).split()).most_common(100)\n",
    "\n",
    "print(most_occur_spam)\n",
    "\n",
    "most_word_spam = []\n",
    "for i in most_occur_spam:\n",
    "    a, b = i\n",
    "    most_word_spam.append(a)\n",
    "print(most_word_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 2305), ('you', 1531), ('to', 1223), ('the', 877), ('a', 828), ('u', 819), ('and', 684), ('in', 643), ('me', 623), ('my', 607), ('is', 567), ('it', 563), ('of', 428), ('that', 421), ('for', 394), ('but', 350), ('s', 344), ('so', 344), ('can', 340), ('have', 334), ('not', 332), ('are', 331), ('your', 329), ('on', 314), ('m', 311), ('at', 304), ('do', 303), ('t', 286), ('we', 277), ('will', 273), ('be', 271), ('if', 266), ('gt', 256), ('2', 254), ('lt', 253), ('how', 251), ('just', 245), ('no', 239), ('get', 238), ('up', 237), ('now', 235), ('when', 226), ('ok', 223), ('this', 213), ('with', 212), ('what', 205), ('ll', 202), ('all', 195), ('good', 194), ('ur', 194), ('go', 193), ('got', 192), ('call', 190), ('like', 189), ('was', 187), ('know', 183), ('then', 180), ('come', 178), ('he', 178), ('or', 177), ('its', 174), ('out', 174), ('am', 170), ('love', 167), ('day', 166), ('time', 161), ('there', 155), ('4', 140), ('she', 136), ('home', 135), ('need', 132), ('lor', 128), ('from', 127), ('sorry', 125), ('going', 125), ('da', 124), ('about', 122), ('want', 122), ('one', 122), ('n', 121), ('still', 119), ('k', 115), ('today', 112), ('as', 111), ('don', 109), ('dont', 109), ('r', 108), ('see', 107), ('by', 105), ('her', 104), ('d', 104), ('only', 104), ('did', 101), ('tell', 100), ('send', 100), ('later', 100), ('take', 100), ('think', 99), ('pls', 98), ('hi', 97)]\n",
      "['i', 'you', 'to', 'the', 'a', 'u', 'and', 'in', 'me', 'my', 'is', 'it', 'of', 'that', 'for', 'but', 's', 'so', 'can', 'have', 'not', 'are', 'your', 'on', 'm', 'at', 'do', 't', 'we', 'will', 'be', 'if', 'gt', '2', 'lt', 'how', 'just', 'no', 'get', 'up', 'now', 'when', 'ok', 'this', 'with', 'what', 'll', 'all', 'good', 'ur', 'go', 'got', 'call', 'like', 'was', 'know', 'then', 'come', 'he', 'or', 'its', 'out', 'am', 'love', 'day', 'time', 'there', '4', 'she', 'home', 'need', 'lor', 'from', 'sorry', 'going', 'da', 'about', 'want', 'one', 'n', 'still', 'k', 'today', 'as', 'don', 'dont', 'r', 'see', 'by', 'her', 'd', 'only', 'did', 'tell', 'send', 'later', 'take', 'think', 'pls', 'hi']\n"
     ]
    }
   ],
   "source": [
    "#find most common ham words\n",
    "\n",
    "most_occur_ham = Counter(\" \".join(allham_messages[\"Text\"]).split()).most_common(100)\n",
    "\n",
    "print(most_occur_ham)\n",
    "\n",
    "most_word_ham = []\n",
    "for i in most_occur_ham:\n",
    "    a, b = i\n",
    "    most_word_ham.append(a)\n",
    "print(most_word_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'get', 'me', 'it', 'out', 'for', 'of', 'on', 'from', 'send', 'we', 'have', 's', 'ur', 'i', 'as', 'all', 'just', 'the', 'this', 'call', 'are', 'u', 'and', '4', 'now', 't', 'no', '2', 'by', 'or', 'only', 'in', 'is', 'be', 'with', 'to', 'a', 'your', 'will']\n"
     ]
    }
   ],
   "source": [
    "#find intersect of list\n",
    "\n",
    "bothspamham = list(set(most_word_spam) & set(most_word_ham))\n",
    "print(bothspamham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"sms_test_dist.csv\")\n",
    "\n",
    "spamham = []\n",
    "for sms in df_test['Text']:\n",
    "    p_wi_spam_sum = 0\n",
    "    p_wi_ham_sum = 0\n",
    "    for word in str(sms).split():\n",
    "        if word in vocabulary:\n",
    "            #don't consider words that are common in both spam and ham\n",
    "            if word not in bothspamham:\n",
    "                p_wi_spam_sum = math.log(p_wi_spam[word]) +  p_wi_spam_sum \n",
    "                p_wi_ham_sum = math.log(p_wi_ham[word]) +  p_wi_ham_sum \n",
    "    p_spam_wi = math.exp(math.log(p_spam) + p_wi_spam_sum)\n",
    "    p_ham_wi = math.exp(math.log(p_ham) + p_wi_ham_sum)\n",
    "    if(p_spam_wi >= p_ham_wi):\n",
    "        spamham.append(\"spam\")\n",
    "    else:\n",
    "        spamham.append(\"ham\")\n",
    "\n",
    "df_test1 = df_test\n",
    "df_test1['Class'] = spamham\n",
    "\n",
    "df_test1.to_csv('full_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
